{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4ae1ebf52760>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#  \"<storage-access-key>\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m spark.conf.set(\n\u001b[0m\u001b[0;32m      7\u001b[0m   \u001b[1;34m\"fs.azure.account.key.databrickstore.blob.core.windows.net\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m   \"S1PtMWvUw5If1Z8FMzXAxC7OMw9G5Go8BGCXJ81qpFVYpZ9dpXOnU4zlg0PbldKkbLIbmbv02WoJsgYLGKIfgg==\")\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up an account access key\n",
    "# spark.conf.set(\n",
    "#  \"fs.azure.account.key.<storage-account-name>.blob.core.windows.net\",\n",
    "#  \"<storage-access-key>\")\n",
    "\n",
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.databrickstore.blob.core.windows.net\",\n",
    "  \"S1PtMWvUw5If1Z8FMzXAxC7OMw9G5Go8BGCXJ81qpFVYpZ9dpXOnU4zlg0PbldKkbLIbmbv02WoJsgYLGKIfgg==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-32da4334f8f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# dbutils.fs.ls(\"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net/<your-directory-name>\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdbutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"wasbs://dbdemo01@databrickstore.blob.core.windows.net\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "# dbutils.fs.ls(\"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net/<your-directory-name>\")\n",
    "dbutils.fs.ls(\"wasbs://dbdemo01@databrickstore.blob.core.windows.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-522784f80ffe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# [note] <mount_point> is a DBFS path and the path must be under /mnt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m dbutils.fs.mount(\n\u001b[0m\u001b[0;32m      9\u001b[0m   \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"wasbs://dbdemo01@databrickstore.blob.core.windows.net\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mmount_point\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/mnt/dbdemo01\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "# mount a Blob storage container or a folder inside a container\n",
    "# dbutils.fs.mount(\n",
    "#   source = \"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net/<your-directory-name>\",\n",
    "#   mount_point = \"<mount-point-path>\",\n",
    "#   extra_configs = <\"<conf-key>\": \"<conf-value>\">)\n",
    "# [note] <mount_point> is a DBFS path and the path must be under /mnt\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = \"wasbs://dbdemo01@databrickstore.blob.core.windows.net\",\n",
    "  mount_point = \"/mnt/dbdemo01\",\n",
    "  extra_configs = {\"fs.azure.account.key.databrickstore.blob.core.windows.net\": \"S1PtMWvUw5If1Z8FMzXAxC7OMw9G5Go8BGCXJ81qpFVYpZ9dpXOnU4zlg0PbldKkbLIbmbv02WoJsgYLGKIfgg==\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-968670698020>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# (JSON) df = spark.read.json(\"/mnt/%s/....\" % <mount-point-path>)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"/mnt/%s/small_radio_json.json\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;34m\"dbdemo01\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# display(df)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Access files in your container as if they were local files\n",
    "# (TEXT) df = spark.read.text(\"/mnt/%s/....\" % <mount-point-path>)\n",
    "# (JSON) df = spark.read.json(\"/mnt/%s/....\" % <mount-point-path>)\n",
    "\n",
    "df = spark.read.json( \"/mnt/%s/small_radio_json.json\" % \"dbdemo01\" )\n",
    "\n",
    "# display(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unmount\n",
    "# dbutils.fs.unmount(\"<mount-point-path>\")\n",
    "# dbutils.fs.unmount(\"/mnt/dbdemo01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d57b1c18404d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspecificColumnsDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"firstname\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lastname\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gender\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"location\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"level\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mspecificColumnsDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "specificColumnsDf = df.select(\"firstname\", \"lastname\", \"gender\", \"location\", \"level\")\n",
    "specificColumnsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'specificColumnsDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9c3d864d8683>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrenamedColumnsDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecificColumnsDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"level\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"subscription_type\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrenamedColumnsDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'specificColumnsDf' is not defined"
     ]
    }
   ],
   "source": [
    "renamedColumnsDF = specificColumnsDf.withColumnRenamed(\"level\", \"subscription_type\")\n",
    "renamedColumnsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply some transformations to the data, then use the\n",
    "# Data Source API to write the data back to another table in SQL DW.\n",
    "\n",
    "# [note] the SQL date warehouse connector uses Azure Blob Storage as a temporary storage to upload data between Azure Databricks and Azure SQL Data Warehouse.\n",
    "\n",
    "## SQL Data Warehouse related settings\n",
    "dwTable= \"mytable001\"\n",
    "dwDatabase = \"sqldwdemo001\"\n",
    "dwServer = \"sqldwdemoserver001\" \n",
    "dwUser = \"yoichika\"\n",
    "dwPass = \"P@ssw0rd____\"\n",
    "dwJdbcPort =  \"1433\"\n",
    "dwJdbcExtraOptions = \"encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n",
    "sqlDwUrl = \"jdbc:sqlserver://\" + dwServer + \".database.windows.net:\" + dwJdbcPort + \";database=\" + dwDatabase + \";user=\" + dwUser+\";password=\" + dwPass + \";$dwJdbcExtraOptions\"\n",
    "sqlDwUrlSmall = \"jdbc:sqlserver://\" + dwServer + \".database.windows.net:\" + dwJdbcPort + \";database=\" + dwDatabase + \";user=\" + dwUser+\";password=\" + dwPass\n",
    "\n",
    "\n",
    "tempDir = \"wasbs://dbdemo01tmp@databrickstore.blob.core.windows.net/tempDirs\"\n",
    "\n",
    "#sc._jsc.hadoopConfiguration().set(\n",
    "#  \"fs.azure.account.key.<your-storage-account-name>.blob.core.windows.net\",\n",
    "#  \"<your-storage-account-access-key>\")\n",
    "acntInfo = \"fs.azure.account.key.databrickstore.blob.core.windows.net\"\n",
    "sc._jsc.hadoopConfiguration().set(\n",
    "  acntInfo, \n",
    "  \"S1PtMWvUw5If1Z8FMzXAxC7OMw9G5Go8BGCXJ81qpFVYpZ9dpXOnU4zlg0PbldKkbLIbmbv02WoJsgYLGKIfgg==\")\n",
    "\n",
    "## Loading transformed dataframe (renamedColumnsDF) into SQLDW\n",
    "spark.conf.set(\"spark.sql.parquet.writeLegacyFormat\",\"true\")\n",
    "\n",
    "## This snippet creates a table called 'dwTable' in the SQL database.\n",
    "#df.write \\\n",
    "#  .format(\"com.databricks.spark.sqldw\") \\\n",
    "#  .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n",
    "#  .option(\"forward_spark_azure_storage_credentials\", \"true\") \\\n",
    "#  .option(\"dbtable\", \"my_table_in_dw_copy\") \\\n",
    "#  .option(\"tempdir\", \"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net/<your-directory-name>\") \\\n",
    "#  .save()\n",
    "\n",
    "renamedColumnsDF.write \\\n",
    "  .format(\"com.databricks.spark.sqldw\") \\\n",
    "  .option(\"url\", sqlDwUrlSmall) \\\n",
    "  .option(\"dbtable\", dwTable) \\\n",
    "  .option( \"forward_spark_azure_storage_credentials\",\"true\") \\\n",
    "  .option(\"tempdir\", tempDir) \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
